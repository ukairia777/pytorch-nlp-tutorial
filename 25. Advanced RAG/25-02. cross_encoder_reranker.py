# -*- coding: utf-8 -*-
"""크로스 인코더 기반의 리랭커 (wikidocs - 25-06-20).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4CV2R-svixZVUX6jeIzEhJG1F5cZnBC
"""

!pip install langchain_openai langchain_community langchain_chroma pypdf

import os
import requests
from typing import List
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain.retrievers import ContextualCompressionRetriever

os.environ['OPENAI_API_KEY'] = '여러분의 키 값'

# 분석할 PDF 파일을 웹에서 다운로드.
url = "https://github.com/llama-index-tutorial/llama-index-tutorial/raw/main/ch07/2023_%EB%B6%81%ED%95%9C%EC%9D%B8%EA%B6%8C%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf"
filename = "2023_북한인권보고서.pdf"

response = requests.get(url)
with open(filename, "wb") as f:
    f.write(response.content)

print(f"{filename} 다운로드 완료")

# LangChain의 LLM과 임베딩 모델 설정
llm = ChatOpenAI(model="gpt-4o", temperature=0.2)  # GPT-4o를 언어 모델로 사용
embed_model = OpenAIEmbeddings(model="text-embedding-3-large")  # 임베딩 모델 사용

# 문서 분할 설정
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,  # 문서를 300자 단위로 분할
    chunk_overlap=100,  # 문맥 유지를 위해 청크 간 100자 중복
)

# PDF 문서를 읽고 벡터 인덱스 생성
loader = PyPDFLoader("2023_북한인권보고서.pdf")  # PDF 문서 로더
documents = loader.load()  # 문서에서 텍스트 추출
chunks = text_splitter.split_documents(documents)  # 문서 분할
vector_store = Chroma.from_documents(chunks, embed_model)  # 추출된 텍스트로 벡터 인덱스 생성

# 기본 검색 엔진 (리랭킹 없음)
basic_retriever = vector_store.as_retriever(search_kwargs={"k": 4})

# Reranker 설정
cross_encoder = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")
reranker = CrossEncoderReranker(model=cross_encoder, top_n=2)

# 리랭킹이 포함된 검색 엔진
rerank_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=basic_retriever
)

# 최종 답변 생성 함수
def generate_answer(query: str, documents: List[Document]) -> str:
    context = "\n\n".join([doc.page_content for doc in documents])

    prompt = f"""다음 검색 결과를 바탕으로 질문에 답변해주세요.
    검색 결과의 정보를 최대한 사용하고, 없는 정보는 답변하지 마세요.

    검색 결과:
    {context}

    질문: {query}

    답변:"""

    response = llm.invoke(prompt)
    return response.content

# 쿼리 실행
query = "19년 말 평양시 소재 기업소에서 달마다 배급받은 음식"

print("=== 기본 검색 엔진 검색 결과 ===")
basic_documents = basic_retriever.invoke(query)
basic_response = generate_answer(query, basic_documents)

print(f"\n질문: {query}")
print(f"답변: {basic_response}")
print("\n검색된 문서:")
for i, doc in enumerate(basic_documents):
    print(f"\n검색 문서 {i+1}:")
    print(doc.page_content)
    print("---")

print("\n\n=== 리랭킹 후 검색 결과 ===")
rerank_documents = rerank_retriever.invoke(query)
rerank_response = generate_answer(query, rerank_documents)

print(f"\n질문: {query}")
print(f"답변: {rerank_response}")
print("\n검색된 문서:")
for i, doc in enumerate(rerank_documents):
    print(f"\n검색 문서 {i+1}:")
    print(doc.page_content)
    print("---")